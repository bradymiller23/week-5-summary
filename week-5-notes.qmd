---
title: "Week 5 Notes"
format: html
---

```{r}
library(tidyverse)
library(ISLR2)
library(cowplot)
library(kableExtra)
```

# **February 7th**

#### **What is the interpretation of $\beta_0$ and $\beta_1$ (regression coefficients)**

The regression model is given as follows:
$$
y_i = \beta_0 + \beta_1*x_i + \epsilon_i
$$
where:

1. $y_i$ are the response
1. $x_i$ is the covariate
1. $\epsilon_i$ is the error (vertical black line in lecture 4 notes)
1. $\beta_0$ and $beta_1$ are the regression coefficients
1. $i=1,2, \dots, n$ are the indices for the observations


* $\beta_0$ is the intercept
* $\beta_1$ is the slope


```{r}
library(ggplot2)
attach(mtcars)

mtcars %>%
  head() %>%
  kable()
```

Consider the following relationship

```{r}
x <- mtcars$hp
y <- mtcars$mpg

plot(x, y, pch = 20, xlab = "HP", ylab = "MPG")

model <- lm(y ~ x)
summary(model)
```
For the intercept this means that:

* A 'hypothetical' car with 'hp=0' will have 'mpg=30.09' = $\beta_0$


Its more instructive and interesting to consider the interpretation of the slope:

Lets say we have some covariate $x_0$ then the expected value for $y(x_0)$ is given by:

* $$y(x_0) = \beta_0 + \beta_1 x_0$$

The expected value for $x_0 + 1$ is...
$$
\begin{align}
y(x_0 + 1) = \beta_0 + \beta_1 \times (x_0 + 1)\\ \\
&= \beta_0 + \beta_1 x_0 + \beta_1\\ \\
&= y(x_0) + \beta_1\\ \\above

\implies \beta_1 &= y(x_0 + 1) - y(x_0)
\end{align}
$$


#### **Cateogorical covariates**

Up until now, we have looked at _simple_ linear regression models where both $x$ and $y$ are quantitative

Confirm that 'cyl' is categorical
```{r}
summary(cyl)
```

Another example is with the iris dataset:
```{r}
iris %>%
  head() %>%
  kable()
```

Want to see if there is a relationship between 'species' and 'sepal.length'

```{r}
x <- iris$Species
y <- iris$Sepal.Length

# boxplot(Sepal.Length ~ Species, df)
boxplot(y ~ x)
```


Lets run a linear regression model and see what the model output is going to look like
```{r}
cat_model <- lm(Sepal.Length ~ Species, iris)
cat_model
```
Even if $x$ is categorical, we can still write down the regression model as follows:
$$
y_i = \beta_0 + \beta_1 x_i
$$
where $x_i \in \{setosa, \ versicolor, \ virginica \}$. This means that we end up with, (fundamentally) three different models

1. $y_i = \beta_0 + \beta_1 (x_i == 'setosa')$
1. $y_i = \beta_0 + \beta_1 (x_i == 'versicolor')$
1. $y_i = \beta_0 + \beta_1 (x_i == 'virginica')$


Now the interpretation for the coefficients are as follows:


**Intercept**

$\beta_0$ is the expected $y$ value when $x$ belongs to the base category. This is what the intercept is capturing

**Slope**

$\beta_1$ with the name 'Species.versicolor' represents the following

'(Intercept)' = $y(x = \texttt{setosa})$

'Species.versicolor' = $y(x = \texttt{versicolor}) - y(x = \texttt{setosa})$
'Species.virginica' = $y(x = \texttt{virginica}) - y(x = \texttt{setosa})$



#### **Reordering the factors**

Lets say that we didn't want 'setosa' to be the baseline level, and instead, we wanted 'virginica' to be the baseline level. How would we do this?

First we reorder/relevel the categorical covariate
```{r}
# before
iris$Species
iris$Species <- relevel(iris$Species, "virginica")

# after
iris$Species
```

Once we do the re-leveling, we can now run the regression model:
```{r}
new_cat_model <- lm(Sepal.Length ~ Species, iris)
new_cat_model
```



# **February 9th**

```{r}
library(plotly)
```


### **Multiple regression**

This is the extension of simple linear regression to multiple covariates
$X = [x_1 | x_2 | \dots | x_p]$, i.e.,

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \epsilon
$$





and the full description of the model is as follows:
$$
y = \beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \dots + \beta_p x_{p,i} + \epsilon
$$

Consider the 'Credit' dataset
```{r}
library(tibble)
library(ISLR2)
attach(Credit)

df <- Credit %>%
  tibble()
colnames(df) <- tolower(colnames(df))
df
```

We will look at the following 3 columns: 'income, rating, limit'.

```{r}
df3 <- df %>%
  select('income', 'rating', 'limit')
df3
```

If we want to see how the credit limit is related to income and credit rating, we can visualize the following plot

```{r}
fig <- plot_ly(df3, x=~income, y=~rating, z=~limit)
fig %>% add_markers()
```
This models a linear relationship, which in 3-dimensions is a plane (a hyperplane)



The regression model is as follows:
```{r}
model <- lm(limit ~ income + rating, df3)
model
```
* Have 3 different coefficients
* 2nd/3rd numbers are the slopes associated with the income and rating

```r
ranges <- df3 %>%
  select(income, rating) %>%
  colnames() %>%

```


What is the interpretation for the coefficients?

1. $\beta_0$ is the expected value of $y$ when $income = 0$ and $rating = 0$ (the credit limit with 0 income and 0 rating is -532) --> this is an extrapolation
1. $\beta_1$ is saying that if $rating$ is held constant and $income$ changes by 1 unit, then the corresponding change in the 'limit' is $0.5573$.
1. $\beta_2$ is saying that if $income$ is held constant and $rating$ changes by 1 unit, then the corresponding change in the 'limit' is $14.7711$.


What about the significance?
```{r}
summary(model)
```
The $p$-value for 'rating' is very significant, while its not significant for 'income'

* Multi-colinearity issue with the 3D model

  1. Not idealisitic that if we hold rating constant and income increases, by one unit then limit increase by 0.5573. --> You can't change income by 1 without impacting rating
 
 
 
 
```r
x <- seq(0,1,100)
b0 <- 1.0
b1 <- 2.0
y <- b0 + b1 * x + rnorm(100) * 0.1
plot(x, y, pch = 20)

model <- lm(y ~ x)
summary(model)
```
* Lower noise increases the $R^2$ value

* If you make $\beta_0$, $\beta_1$, and noise values super small, then p-value will be super high and lead to a low $R^2$ value

* Can have high $p$-value with low $R^2$ value, but CANT have low $p$-value with high $R^2$ value



#### Multiple Regression with categorical covariates
Using income, rating, and student status to predict limit you will run 2 different models ... 

1. once for when student is yes
1. once for when student is no

* By adding the categorical variable, you are adding an additional intercept term

```r
model <- lm(limit ~ rating + married, df)

ggplot(df) +
  geom_point(aes(x=rating, y=limit, color=married)) +
  geom_smooth(aes(x=rating, y=limit, fill=married))

```




